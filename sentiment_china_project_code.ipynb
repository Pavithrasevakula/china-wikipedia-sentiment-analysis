{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61755355",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from textblob import TextBlob\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43584c0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sevak\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\sevak\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Downloading necessary NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa88feb7",
   "metadata": {},
   "source": [
    "Web scrape China Wikipedia page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42952619",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_wikipedia(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Extract text from paragraphs in the main content\n",
    "    content = soup.find('div', {'id': 'mw-content-text'})\n",
    "    paragraphs = content.find_all('p')\n",
    "    \n",
    "    # Combine all paragraphs into one text\n",
    "    text = ' '.join([p.get_text() for p in paragraphs])\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4f65b0",
   "metadata": {},
   "source": [
    "Clean and preprocess text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c906c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # Remove references [1], [2], etc.\n",
    "    text = re.sub(r'\\[\\d+\\]', '', text)\n",
    "    \n",
    "    # Remove special characters and extra whitespace\n",
    "    text = re.sub(r'[^\\w\\s.,;:!?]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035ada91",
   "metadata": {},
   "source": [
    "Tokenize sentences and analyze sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bcc3b544",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_sentiment(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    \n",
    "    sentiment_data = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        if len(sentence.strip()) > 10:  # Ignore very short segments\n",
    "            analysis = TextBlob(sentence)\n",
    "            polarity = analysis.sentiment.polarity\n",
    "            \n",
    "            # Classify sentiment\n",
    "            if polarity > 0.0:\n",
    "                sentiment = 'positive'\n",
    "            elif polarity < 0.0:\n",
    "                sentiment = 'negative'\n",
    "            else:\n",
    "                sentiment = 'neutral'\n",
    "            \n",
    "            sentiment_data.append({\n",
    "                'sentence': sentence,\n",
    "                'polarity': polarity,\n",
    "                'sentiment': sentiment\n",
    "            })\n",
    "    \n",
    "    return sentiment_data, sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98cac646",
   "metadata": {},
   "source": [
    "Word tokenize and remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "929cd6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_words(text):\n",
    "    words = word_tokenize(text.lower())\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_words = [word for word in words if word.isalnum() and word not in stop_words]\n",
    "    return filtered_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2efcd47f",
   "metadata": {},
   "source": [
    "Create wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1bb4b4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_wordcloud(words):\n",
    "    word_freq = Counter(words)\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(word_freq)\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('china_wordcloud.png')\n",
    "    plt.close()\n",
    "    \n",
    "    return word_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f85f07e",
   "metadata": {},
   "source": [
    "Build and evaluate machine learning models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb2ee2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_models(X_train, X_test, y_train, y_test):\n",
    "    models = {\n",
    "        'Logistic Regression': LogisticRegression(max_iter=1000),\n",
    "        'Decision Tree': DecisionTreeClassifier(),\n",
    "        'Random Forest': RandomForestClassifier(),\n",
    "        'Gradient Boosting': GradientBoostingClassifier(),\n",
    "        'Naive Bayes': MultinomialNB(),\n",
    "        'K-Nearest Neighbors': KNeighborsClassifier()\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    best_f1 = 0\n",
    "    best_model = None\n",
    "    best_model_name = None\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "        \n",
    "        results[name] = {\n",
    "            'accuracy': accuracy,\n",
    "            'f1_score': f1,\n",
    "            'model': model\n",
    "        }\n",
    "        \n",
    "        print(f\"{name}:\")\n",
    "        print(f\"  Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"  F1 Score: {f1:.4f}\")\n",
    "        print(classification_report(y_test, y_pred))\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_model = model\n",
    "            best_model_name = name\n",
    "    \n",
    "    print(f\"Best model: {best_model_name} with F1 Score of {best_f1:.4f}\")\n",
    "    \n",
    "    return best_model, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c470872d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Web scrape Wikipedia page of China\n",
    "    url = \"https://en.wikipedia.org/wiki/China\"\n",
    "    raw_text = scrape_wikipedia(url)\n",
    "    \n",
    "    # Clean Text and preprocessing\n",
    "    cleaned_text = clean_text(raw_text)\n",
    "    \n",
    "    # Sentence tokenize and sentiment analysis\n",
    "    sentiment_data, sentences = analyze_sentiment(cleaned_text)\n",
    "    \n",
    "    # Create a dataframe of sentences and sentiment\n",
    "    df = pd.DataFrame(sentiment_data)\n",
    "    print(f\"Total sentences: {len(df)}\")\n",
    "    print(df['sentiment'].value_counts())\n",
    "    \n",
    "    #  Word tokenize and remove stopwords\n",
    "    words = process_words(cleaned_text)\n",
    "    \n",
    "    #  Generate wordcloud\n",
    "    word_freq = generate_wordcloud(words)\n",
    "    \n",
    "    # Find frequent words\n",
    "    print(\"\\n--- Most Common Words ---\")\n",
    "    for word, count in word_freq.most_common(30):\n",
    "        print(f\"{word}: {count}\")\n",
    "    \n",
    "    # Apply TFIDF Vectorizer on sentences\n",
    "    df_binary = df[df['sentiment'] != 'neutral'].copy()  #  Use only positive and negative sentiments\n",
    "    \n",
    "    print(\"\\n--- After removing neutral sentiments ---\")\n",
    "    print(df_binary['sentiment'].value_counts())\n",
    "    \n",
    "    # Split into features and target\n",
    "    X = df_binary['sentence']\n",
    "    y = df_binary['sentiment']\n",
    "    \n",
    "    # Apply TF-IDF Vectorization\n",
    "    tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
    "    X_tfidf = tfidf_vectorizer.fit_transform(X)\n",
    "    \n",
    "    # Split into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    #  Use SMOTE to balance sentiment\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "    \n",
    "    print(\"\\n--- Class distribution after SMOTE ---\")\n",
    "    print(pd.Series(y_train_resampled).value_counts())\n",
    "    \n",
    "    #  Build machine learning models\n",
    "    best_model, results = build_models(X_train_resampled, X_test, y_train_resampled, y_test)\n",
    "    \n",
    "    # Saving the best model and vectorizer\n",
    "    with open('best_model.pkl', 'wb') as f:\n",
    "        pickle.dump(best_model, f)\n",
    "    \n",
    "    with open('tfidf_vectorizer.pkl', 'wb') as f:\n",
    "        pickle.dump(tfidf_vectorizer, f)\n",
    "    \n",
    "    print(\"Best model and vectorizer saved to files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "641ff9bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sentences: 610\n",
      "sentiment\n",
      "positive    265\n",
      "neutral     231\n",
      "negative    114\n",
      "Name: count, dtype: int64\n",
      "\n",
      "--- Most Common Words ---\n",
      "china: 324\n",
      "chinese: 134\n",
      "world: 112\n",
      "largest: 72\n",
      "country: 64\n",
      "million: 43\n",
      "one: 42\n",
      "dynasty: 41\n",
      "population: 40\n",
      "government: 40\n",
      "state: 34\n",
      "also: 32\n",
      "ccp: 31\n",
      "since: 30\n",
      "second: 28\n",
      "first: 25\n",
      "people: 24\n",
      "according: 24\n",
      "national: 24\n",
      "update: 24\n",
      "2023: 23\n",
      "asia: 22\n",
      "three: 22\n",
      "east: 21\n",
      "states: 21\n",
      "major: 21\n",
      "total: 21\n",
      "prc: 20\n",
      "billion: 20\n",
      "beijing: 20\n",
      "\n",
      "--- After removing neutral sentiments ---\n",
      "sentiment\n",
      "positive    265\n",
      "negative    114\n",
      "Name: count, dtype: int64\n",
      "\n",
      "--- Class distribution after SMOTE ---\n",
      "sentiment\n",
      "positive    182\n",
      "negative    182\n",
      "Name: count, dtype: int64\n",
      "Logistic Regression:\n",
      "  Accuracy: 0.7632\n",
      "  F1 Score: 0.7352\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.62      0.32      0.43        31\n",
      "    positive       0.79      0.93      0.85        83\n",
      "\n",
      "    accuracy                           0.76       114\n",
      "   macro avg       0.71      0.63      0.64       114\n",
      "weighted avg       0.74      0.76      0.74       114\n",
      "\n",
      "--------------------------------------------------\n",
      "Decision Tree:\n",
      "  Accuracy: 0.5702\n",
      "  F1 Score: 0.5851\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.28      0.35      0.31        31\n",
      "    positive       0.73      0.65      0.69        83\n",
      "\n",
      "    accuracy                           0.57       114\n",
      "   macro avg       0.50      0.50      0.50       114\n",
      "weighted avg       0.61      0.57      0.59       114\n",
      "\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sevak\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest:\n",
      "  Accuracy: 0.7719\n",
      "  F1 Score: 0.7299\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.73      0.26      0.38        31\n",
      "    positive       0.78      0.96      0.86        83\n",
      "\n",
      "    accuracy                           0.77       114\n",
      "   macro avg       0.75      0.61      0.62       114\n",
      "weighted avg       0.76      0.77      0.73       114\n",
      "\n",
      "--------------------------------------------------\n",
      "Gradient Boosting:\n",
      "  Accuracy: 0.7281\n",
      "  F1 Score: 0.7201\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.50      0.42      0.46        31\n",
      "    positive       0.80      0.84      0.82        83\n",
      "\n",
      "    accuracy                           0.73       114\n",
      "   macro avg       0.65      0.63      0.64       114\n",
      "weighted avg       0.72      0.73      0.72       114\n",
      "\n",
      "--------------------------------------------------\n",
      "Naive Bayes:\n",
      "  Accuracy: 0.7018\n",
      "  F1 Score: 0.7129\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.46      0.61      0.53        31\n",
      "    positive       0.84      0.73      0.78        83\n",
      "\n",
      "    accuracy                           0.70       114\n",
      "   macro avg       0.65      0.67      0.65       114\n",
      "weighted avg       0.73      0.70      0.71       114\n",
      "\n",
      "--------------------------------------------------\n",
      "K-Nearest Neighbors:\n",
      "  Accuracy: 0.2719\n",
      "  F1 Score: 0.1163\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.27      1.00      0.43        31\n",
      "    positive       0.00      0.00      0.00        83\n",
      "\n",
      "    accuracy                           0.27       114\n",
      "   macro avg       0.14      0.50      0.21       114\n",
      "weighted avg       0.07      0.27      0.12       114\n",
      "\n",
      "--------------------------------------------------\n",
      "Best model: Logistic Regression with F1 Score of 0.7352\n",
      "Best model and vectorizer saved to files.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sevak\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\sevak\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\sevak\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe117fb6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
